{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcnp2lE5+RoJUrWF4BHSwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valerii3/OA_CodeReviewer/blob/main/CodeReviewer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automating Code Review Activities\n",
        "---\n",
        "\n",
        "# Introduction:\n",
        "\n",
        "Code review is an integral part of the software development cycle, ensuring the code's quality, functionality, and maintainability. The paper \"Automating Code Review Activities by Large-Scale Pre-training from FSE 2022\" introduces the CodeReviewer model, aiming to automate the process by leveraging a pre-trained model on a vast dataset.\n",
        "\n",
        "This notebook aims to implement and evaluate the CodeReviewer model on a chosen GitHub repository."
      ],
      "metadata": {
        "id": "BqUT88Syj25D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup:\n",
        "Before we proceed, let's install all the required dependencies:"
      ],
      "metadata": {
        "id": "zAJ6ICGAkNOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjJr2FmwZS0O"
      },
      "outputs": [],
      "source": [
        "!pip install requests\n",
        "!pip install PyGithub\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install nltk\n",
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Diff Hunks using Special Tokens\n",
        "\n",
        "In the research paper, the authors highlighted the significance of preprocessing the data for enhancing the model's efficiency. They introduced a unique approach by substituting the traditional symbols used in diff hunks (like +, -, and space) with specific tokens such as ADD, DEL, and KEEP. This method, as per the paper, resulted in improved model performance.\n",
        "\n",
        "The following function, sourced from the authors' GitHub repository, implements this preprocessing:"
      ],
      "metadata": {
        "id": "A9Py821zvhzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_special_tokens(diff_hunk: str):\n",
        "    diff_lines = diff_hunk.split(\"\\n\")[1:]        # remove start @@\n",
        "    diff_lines = [line for line in diff_lines if len(line.strip()) > 0]\n",
        "    map_dic = {\"-\": 0, \"+\": 1, \" \": 2}\n",
        "    def f(s):\n",
        "        if s in map_dic:\n",
        "            return map_dic[s]\n",
        "        else:\n",
        "            return 2\n",
        "    labels = [f(line[0]) for line in diff_lines]\n",
        "    diff_lines = [line[1:].strip() for line in diff_lines]\n",
        "    input_str = \"\"\n",
        "    for label, line in zip(labels, diff_lines):\n",
        "        if label == 1:\n",
        "            input_str += \"<add>\" + line\n",
        "        elif label == 0:\n",
        "            input_str += \"<del>\" + line\n",
        "        else:\n",
        "            input_str += \"<keep>\" + line\n",
        "    return input_str\n"
      ],
      "metadata": {
        "id": "htxg7OzTvnMD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting Review Comments from the pandas Repository\n",
        "\n",
        "For the purpose of our analysis, it's crucial to have a dataset that's representative of real-world code review comments. Given the stature and the quality of the code in the pandas repository, it serves as an ideal candidate for our data collection.\n",
        "\n",
        "We are particularly interested in recent review comments to ensure our analysis is based on the most up-to-date practices.\n",
        "\n",
        "To efficiently gather this data, the PyGithub library is employed, which provides a seamless interface to interact with the GitHub API using Python.\n",
        "\n",
        "This code facilitates the extraction of the most recent 100 review comments from the pandas repository. It focuses on Python files and ensures the uniqueness of the diffs for a fair comparison between generated and actual comments."
      ],
      "metadata": {
        "id": "VV-4to9wZeEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github\n",
        "\n",
        "TOKEN = \"TOKEN\"  # Replace with your token\n",
        "\n",
        "# Initialize the Github object with the token\n",
        "g = Github(TOKEN)\n",
        "\n",
        "repo = g.get_repo(\"pandas-dev/pandas\")\n",
        "\n",
        "# Fetch recent pull requests\n",
        "pulls = repo.get_pulls(state=\"open\", sort=\"created\", direction=\"desc\")  # Fetching open PRs sorted by creation time in descending order\n",
        "\n",
        "filtered_comments = []\n",
        "processed_diffs = set()  # To keep track of the processed hunks\n",
        "\n",
        "# Iterate through the PRs and fetch review comments\n",
        "it = 0\n",
        "for pr in pulls:\n",
        "    comments = pr.get_review_comments()\n",
        "    for comment in comments:\n",
        "        if not comment.path.endswith('.py'):  # Check if the file is a Python file\n",
        "            continue\n",
        "        # Check if comment has no replies and its patch is <= 1000 characters\n",
        "        if comment.in_reply_to_id is None and len(comment.diff_hunk) <= 1000:\n",
        "            preprocessed_patch = add_special_tokens(comment.diff_hunk)\n",
        "\n",
        "            # Check if this diff has been processed before\n",
        "            if preprocessed_patch in processed_diffs:\n",
        "                continue  # Skip this diff\n",
        "\n",
        "            filtered_comments.append({\n",
        "                \"patch\": preprocessed_patch,\n",
        "                \"msg\": comment.body,\n",
        "                \"id\": comment.id,\n",
        "                \"file\": comment.path\n",
        "            })\n",
        "\n",
        "            # Add this diff to the set of processed diffs\n",
        "            processed_diffs.add(preprocessed_patch)\n",
        "\n",
        "        it += 1\n",
        "        if it == 100:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "drs5V5QVf5bH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our project, this model will be harnessed to generate review comments for the preprocessed diffs obtained from the pandas repository.\n"
      ],
      "metadata": {
        "id": "zy287Z3SvoF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Load the model and perform inference\n",
        "pipe = pipeline(\"text2text-generation\", \"microsoft/codereviewer\", max_length=200)\n",
        "\n",
        "preprocessed_diffs = [item[\"patch\"] for item in filtered_comments]\n",
        "generated_comments = pipe(preprocessed_diffs)\n",
        "\n",
        "# Storing the results\n",
        "predictions_and_targets = []\n",
        "for i, output in enumerate(generated_comments):\n",
        "    pred_and_target = {\n",
        "        \"pred\": output['generated_text'],\n",
        "        \"target\": filtered_comments[i]['msg'],\n",
        "        \"id\": filtered_comments[i]['id']\n",
        "    }\n",
        "    predictions_and_targets.append(pred_and_target)\n"
      ],
      "metadata": {
        "id": "oWv1kX-wvp70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the results\n"
      ],
      "metadata": {
        "id": "zSPa0NWJAytL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data in predictions_and_targets[90:95]:\n",
        "    original_diff = [item[\"patch\"] for item in filtered_comments if item[\"id\"] == data[\"id\"]][0]\n",
        "    print(f\"Diff Code (Preprocessed):\\n{original_diff}\\n\")\n",
        "    print(f\"Original Comment: {data['target']}\\n\")\n",
        "    print(f\"Generated Comment: {data['pred']}\\n\")\n",
        "    print(\"< ================ >\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNDWxe4QA0et",
        "outputId": "f51b71ff-36da-4e7b-a223-6bb6b27b0d88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diff Code (Preprocessed):\n",
            "<keep>def make_na_array(dtype: DtypeObj, shape: Shape, fill_value) -> ArrayLike:<keep>if isinstance(dtype, DatetimeTZDtype):<keep># NB: exclude e.g. pyarrow[dt64tz] dtypes<del>i8values = np.full(shape, fill_value._value)<add>i8values = np.full(shape, Timestamp(fill_value)._value)\n",
            "\n",
            "Original Comment: we need to be sure that the Timestamp here has the same unit as the dtype\n",
            "\n",
            "Generated Comment: <msg>This is the only place where we need to convert a `Timestamp` to a `DatetimeTZDtype`.\n",
            "\n",
            "< ================ >\n",
            "Diff Code (Preprocessed):\n",
            "<keep>os.chdir(dirname)<keep>self._run_os(\"zip\", zip_fname, \"-r\", \"-q\", *fnames)<add>def _linkcheck(self):<add>\"\"\"<add>Check for broken links in the documentation.<add>\"\"\"<add>cmd = [\"sphinx-build\", \"-b\", \"linkcheck\"]<add>if self.num_jobs:<add>cmd += [\"-j\", self.num_jobs]<add>if self.verbosity:<add>cmd.append(f\"-{'v' * self.verbosity}\")<add>cmd += [<add>\"-d\",<add>os.path.join(BUILD_PATH, \"doctrees\"),<add>SOURCE_PATH,<add>os.path.join(BUILD_PATH, \"linkcheck\"),<add>]<add>subprocess.call(cmd)<add><keep>def main():<keep>cmds = [method for method in dir(DocBuilder) if not method.startswith(\"_\")]<add>cmds.append(\"linkcheck\")  # Add linkcheck to the available commands\n",
            "\n",
            "Original Comment: This should not be needed. Is there any reason to prefix the method with a `_`? That is the reason it is not picked up automatically.\n",
            "\n",
            "Generated Comment: <msg>I think we should add `-j` to `linkcheck` as well.\n",
            "\n",
            "< ================ >\n",
            "Diff Code (Preprocessed):\n",
            "<keep>os.chdir(dirname)<keep>self._run_os(\"zip\", zip_fname, \"-r\", \"-q\", *fnames)<add>def _linkcheck(self):<add>\"\"\"<add>Check for broken links in the documentation.<add>\"\"\"<add>cmd = [\"sphinx-build\", \"-b\", \"linkcheck\"]<add>if self.num_jobs:<add>cmd += [\"-j\", self.num_jobs]<add>if self.verbosity:<add>cmd.append(f\"-{'v' * self.verbosity}\")<add>cmd += [<add>\"-d\",<add>os.path.join(BUILD_PATH, \"doctrees\"),<add>SOURCE_PATH,<add>os.path.join(BUILD_PATH, \"linkcheck\"),<add>]\n",
            "\n",
            "Original Comment: Not sure about all of this. This comes from the regular HTML build, right? I don't think we need that here and just the regular `cmd` is sufficient.\n",
            "\n",
            "Generated Comment: <msg>I think we should use `sphinx-build -b linkcheck` here.\n",
            "\n",
            "< ================ >\n",
            "Diff Code (Preprocessed):\n",
            "<keep>args.verbosity,<keep>args.warnings_are_errors,<keep>)<del>return getattr(builder, args.command)()<del><add>if args.command == \"linkcheck\":<add>builder._linkcheck()  # Call the linkcheck method<add>else:<add>return getattr(builder, args.command)()<add>\n",
            "\n",
            "Original Comment: Removing the `_` from the method will also make this obsolete.\n",
            "\n",
            "Generated Comment: <msg>I think this should be `if args.command == \"linkcheck\":`\n",
            "\n",
            "< ================ >\n",
            "Diff Code (Preprocessed):\n",
            "<keep>args.verbosity,<keep>args.warnings_are_errors,<keep>)<del>return getattr(builder, args.command)()<add><add>if args.command == \"linkcheck\":<add>builder.linkcheck()  # Call the linkcheck method\n",
            "\n",
            "Original Comment: ```suggestion\r\n",
            "        builder.linkcheck()\r\n",
            "```\n",
            "\n",
            "Generated Comment: <msg>I think this should be `if args.command == \"linkcheck\":`\n",
            "\n",
            "< ================ >\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualitative Analysis Summary:\n",
        "\n",
        "* **Relevance to Original Comment**:\n",
        "\n",
        "  The model's generated comments sometimes align with the intent of the original comments but might express the idea differently.\n",
        "  There are instances where the model's focus deviates from the core concern of the original comment.\n",
        "\n",
        "* **Tendency for Specific Code Suggestions**:\n",
        "\n",
        "  The model frequently provides specific code suggestions or fixes, even when the original comment might be addressing broader concerns or general observations.\n",
        "\n",
        "* **Accuracy Categories**:\n",
        "\n",
        "  Accurate: The model's comment aligns well with the intent of the original comment.\n",
        "\n",
        "  Partially Accurate: The model captures some essence but misses certain key aspects.\n",
        "\n",
        "  Inaccurate: The model's comment deviates significantly from the original or focuses on a different aspect of the code.\n",
        "\n",
        "* **General Observation**:\n",
        "\n",
        "  The model appears to understand code context to some extent, but there's room for improvement in aligning its responses with human reviewers' insights."
      ],
      "metadata": {
        "id": "LRu_tyC0pD_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantitative Analysis\n",
        "\n",
        "For a quantitative measure of how well the generated comments match with the original ones, we employ the BLEU score metric.\n",
        "\n",
        "Originally devised for evaluating machine translations, the BLEU score is also versatile enough to be applied in various text generation tasks, including our case of auto-generated code comments."
      ],
      "metadata": {
        "id": "QgVDUdTeptqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Assuming predictions_and_targets is your data\n",
        "original_comments = [item['target'] for item in predictions_and_targets]\n",
        "generated_comments = [item['pred'] for item in predictions_and_targets]\n",
        "\n",
        "# Compute BLEU scores\n",
        "bleu_scores = []\n",
        "\n",
        "smoother = SmoothingFunction()\n",
        "\n",
        "for original, generated in zip(original_comments, generated_comments):\n",
        "    reference = [original.split()]  # the reference and candidate should be tokenized\n",
        "    candidate = generated.split()\n",
        "    score = sentence_bleu(reference, candidate, smoothing_function=smoother.method1)  # Using smoothing since BLEU can be sensitive to short texts\n",
        "    bleu_scores.append(score)\n",
        "\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "print(f'Average BLEU score for the generated comments: {average_bleu:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqQYbrhhp-vi",
        "outputId": "27cdb694-89d2-44e6-81f2-70a689108d09"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score for the generated comments: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU Metric Analysis\n",
        "\n",
        "Our obtained average BLEU score of 0.0092 suggests a low degree of word overlap between the generated and original comments. While BLEU is efficient in capturing exact word matches, it doesn't necessarily reflect the semantic accuracy or the quality of the content in relation to the context.\n",
        "\n",
        "Given the nuances of our task and the inherent limitations of BLEU, it's crucial to employ another evaluation metric to gain a more comprehensive insight into the performance of the CodeReviewer model. This leads us to the ROUGE metric.\n",
        "\n",
        " By using ROUGE, we aim to get a broader understanding of how closely the generated review comments match the original ones not just in terms of exact word matches, but also in structure and content."
      ],
      "metadata": {
        "id": "TgtvoEairmxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "original_comments = [item[\"target\"] for item in predictions_and_targets]\n",
        "generated_comments_list = [item[\"pred\"] for item in predictions_and_targets]\n",
        "\n",
        "generated_comments_list = [comment.strip() for comment in generated_comments_list if comment.strip()]\n",
        "original_comments = [comment.strip() for comment in original_comments if comment.strip()]\n",
        "\n",
        "# Trim both lists to only have the first 50 comments\n",
        "generated_comments_list = generated_comments_list[:100]\n",
        "original_comments = original_comments[:100]\n",
        "\n",
        "# Now compute the ROUGE scores\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(generated_comments_list, original_comments, avg=True)\n",
        "\n",
        "# Printing the results\n",
        "print(f\"ROUGE-1: {scores['rouge-1']}\")\n",
        "print(f\"ROUGE-2: {scores['rouge-2']}\")\n",
        "print(f\"ROUGE-L: {scores['rouge-l']}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPyiuc7xwOjr",
        "outputId": "75778111-ac5e-44bf-da2f-31d44c9668f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE-1: {'r': 0.05995802890457184, 'p': 0.10664973101611024, 'f': 0.0683260733614867}\n",
            "ROUGE-2: {'r': 0.0024125541125541126, 'p': 0.008469696969696969, 'f': 0.0036003641603207935}\n",
            "ROUGE-L: {'r': 0.057734406122125534, 'p': 0.10168941355579281, 'f': 0.06537170546775711}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  # ROUGE-1 (Unigrams):\n",
        "  Recall (r): About 5.995% of the words in the reference comments are also present in the generated comments.\n",
        "  \n",
        "  Precision (p): About 10.665% of the words in the generated comments are also present in the reference comments.\n",
        "  \n",
        "  F1-score (f): The harmonic mean of precision and recall is about 6.833%.\n",
        "  \n",
        "  # ROUGE-2 (Bigrams):\n",
        "Recall (r): About 0.241% of the bigrams in the reference comments are also present in the generated comments.\n",
        "\n",
        " Precision (p): About 0.847% of the bigrams in the generated comments are also present in the reference comments.\n",
        "\n",
        " F1-score (f): The harmonic mean of precision and recall is about 0.36%.\n",
        "\n",
        "# ROUGE-L (Longest Common Subsequence):\n",
        "\n",
        "Recall (r): The longest common subsequence makes up about 5.773% of the reference comments.\n",
        "\n",
        "Precision (p): The longest common subsequence makes up about 10.169% of the generated comments.\n",
        "\n",
        "F1-score (f): The harmonic mean of precision and recall is about 6.537%."
      ],
      "metadata": {
        "id": "X5xl5XqIyw0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Summary: Code Review Comment Generation and Analysis\n",
        "---\n",
        "# Objective:\n",
        " * Automate the generation of code review comments using machine learning.\n",
        "\n",
        " * Evaluate the quality of the generated comments against actual review comments from the pandas GitHub repository.\n",
        "\n",
        "# Data Collection:\n",
        " * Utilized the PyGithub library to fetch the last 100 review comments from pandas-dev/pandas repository.\n",
        " * Filtered reviews specifically targeting Python files, unique diffs, and diffs not exceeding 1000 characters.\n",
        "\n",
        "# Model for Comment Generation:\n",
        "* Employed the model microsoft/codereviewer from the transformers library to predict comments based on preprocessed diff patches.\n",
        "* Generated comments for the selected diffs.\n",
        "\n",
        "# Qualitative Analysis:\n",
        " * Compared original and generated comments for a few sample diffs.\n",
        " * Observations:\n",
        "      * Some generated comments were relevant and captured the essence of the original comments, but there were also discrepancies.\n",
        "      * In certain cases, the generated comments diverged from the context or missed certain nuances.\n",
        "\n",
        "# Quantitative Analysis:\n",
        "\n",
        "* BLEU Score:\n",
        "  * Used to evaluate the similarity of the generated comments with the original ones.\n",
        "  * Average BLEU score: 0.0092, which indicates a low overlap between the generated comments and the original comments.\n",
        "\n",
        "* ROUGE Score:\n",
        "  * Implemented to capture word and n-gram overlaps and evaluate more detailed content similarities.\n",
        "   * Results:\n",
        "                ROUGE-1: r: 5.995%, p: 10.665%, f: 6.833%\n",
        "                ROUGE-2: r: 0.241%, p: 0.847%, f: 0.36%\n",
        "                ROUGE-L: r: 5.773%, p: 10.169%, f: 6.537%\n",
        "   * The ROUGE scores further highlighted the minimal overlap between generated and reference comments, particularly in terms of phrasing and structure.\n",
        "\n",
        "# Conclusion:\n",
        " * The machine learning model showed potential in generating code review comments.\n",
        " * However, there's room for improvement, given the low BLEU and ROUGE scores.\n",
        " * Further refinements, more extensive training, or additional context might enhance the model's performance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mg2IDC6Tz4rN"
      }
    }
  ]
}